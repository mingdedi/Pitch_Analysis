![License](https://img.shields.io/badge/license-MIT-blue) ![Downloads](https://img.shields.io/pypi/dm/your-package-name)
# Pitch_Analysis
CREPE项目复刻

# CREPE简介
Convolutional Representation for Pitch Estimation这是CREPE的全称。
基于卷积神经网络的基频检测器，数据驱动的算法，直接在空间域进行检测
这个模型训练之初就只针对单频道的音频

过去有很多算法可以用来检测基频，最经典的比如YIN算法，pYIN在基频检测方面取得了十分好的性能。这些方法都基于预先设计好的复杂的候选生成函数以及复杂处理步骤，都需要手动的超参数调节。  
以上的算法基本上都是基于启发式的方法。启发式的方法是一种基于经验和直觉的策略，用于快速找到问题的可接受解决方案，而不保证是最优解。这种方法通常用于复杂问题，提供一种实用而高效的解决方案。通常具有优秀的可解释性。  
启发式方法为什么有用？一种可能的解释是，由于基频是音频信号的低级物理属性，直接与其周期性相关，在许多情况下，用于估计这种周期性的启发式方法表现极佳，其准确率（以后将定义为原始音高准确率）接近100%，这使得一些人认为这个任务已经是一个解决了的问题。  
但是传统的启发式算法，比如pYIN在面对有噪声干扰的基频检测或者变化剧烈的基频检测时，仍然会有一定的问题
# CREPE基本结构与原理
CREPE使用卷积层直接处理信号，模型共有6个卷积层。
## 具体结构
特征输入如下：
首先输入的是一个长度为1024的一维音频强度数据，采样频率为16kHz，持续时间为64ms，  
网络结构如下：
1. 首先是一个1维卷积层，卷积核大小为512，步长为4，不进行填充。最大池化层大小为2，步长为1，不进行填充。输出长度为128，1024维度的数据。
2. 然后是一个1维卷积层，卷积核大小为64，步长为1，不进行填充。最大池化层大小为2，步长为1，不进行填充。输出长度为64，128维度的数据。
3. 然后是一个1维卷积层，卷积核大小为32（但是论文中居然写的是64，难道在这里进行填充？我不理解。。。但是按照输出来讲32大小的卷积核更合理），步长为1，不进行填充。最大池化层大小为2，步长为1，不进行填充。输出长度为32，128维度的数据。
4. 然后是一个1维卷积层，卷积核大小为16（这里我也改了，论文里写的是64），步长为1，不进行填充。最大池化层大小为2，步长为1，不进行填充。输出长度为16，128维度的数据。
5. 然后是一个1维卷积层，卷积核大小为8（这里我也改了，论文里写的是64），步长为1，不进行填充。最大池化层大小为2，步长为1，不进行填充。输出长度为8，256维度的数据。
6. 然后是一个1维卷积层，卷积核大小为4（这里我也改了，论文里写的是64），步长为1，不进行填充。最大池化层大小为2，步长为1，不进行填充。输出长度为4，512维度的数据。
7. 然后重新排列形状，变成一个表达在隐藏空间中的长度为2048的一维数据，然后经过一个全连接层，得到长度为360的特征输出。
8. 然后最后就是使用的sigmoid激活函数，得到最终的预测输出$\hat{y}$
需要注意的是，论文中说，需要自行将目标值进行调整，使得模型尽量朝着一个正态分布的方向去拟合

每个卷积层都进行了归一化。  
并且使用了dropout层防止过拟合，概率为0.25
论文中使用的是Keras，基于tensorflow的，但是复刻用的是pytorch

不知道为什么，论文中没有提到需要在每个卷积层后面放什么激活函数，只提到了需要进行归一化，但是我在实际训练中在卷积层以及归一化层后面放一个RELU激活函数，如果不放的话模型可能表达能力不足。究竟足不足我也不知道，没实验过。

## 模型输出
这个模型的输出一共有360个，每个都带表了一个音高，第一个节点可以认为是$C_i=2000$音分，然后每个节点相隔20音分，到最后一个节点是$C_{360}=9180$音分，但是需要注意的是360个节点输出代表的是置信度，可以理解为基频在该音分范围内的置信度为多少，需要根据以下公式计算最终的音高预测值。也可以转化为频率得到基频。
$$
\hat{C}=\frac{\sum^{360}_{i=1}\hat{y_i}C_i}{\sum^{360}_{i=1}\hat{y_i}},f=f_{ref}2^{\hat{C}/1200}
$$


# 损失函数相关
## 基础乐理知识
这部分做个掌握即可，要明白音分、音程、音高的定义以及关系。当然了我也不是很懂乐理，所以以下内容很可能会有错误的。
### 音分
音分是一种计量单位，是表示和对比音高及音程的相对标准的方法，是的，音分（Cent）是用来衡量音程的单位。  
音分将一个半音分为100等份，因此一个八度有1200音分。音分的定义使得它非常适合精确比较音高的微小差异。

音分的计算公式是：
$$C=1200log_2\frac{f1}{f2}$$

其中f_1和f_2是两个频率。这种度量方式使得音分成为衡量音程的标准化单位，无论音高如何变化都适用。
### 音高
音高（Pitch），是指各种音调高低不同的声音，即音的高度，  
我的理解是音高是音的频率？？  
音高和声音频率有直接关系。音高是我们对声音的感知特性，通常与声音的频率有关。频率越高，音高就越高；频率越低，音高就越低。
在绝大多数情况下，频率可视作是音高的唯一决定因素。
### 音程
音程（pitch interval）是人类感受到两个音音高之间的差距。在人类听力的中频段，人类对不同音高音符之间音程的感知大致是对数的；在高频段则不太符合对数规则。人类听力的这一特性是由人的听觉系统的生理结构决定的。
我的理解是音程表示的是人**主观**对于音高差距大小的感受？？
这也就是说，频率分别为 200Hz/400Hz/800Hz 的三个音，其中两段音程大致是相等的；而频率分别为 200Hz/400Hz/600Hz 的三个音，人类听起这两段音程则不相等。请注意两个音高之间的音程分为八度音程和非八度音程，只有八度音程之间有1200个音分，根据十二平均律，包含了十二个半音。每个半音是100个音分，因此整个八度是1200个音分。

对于频率相差一倍的两个音，例如 440Hz/880Hz，其间的音程定为八度。  
按照定义，一个八度音程的频率比为 2:1，其间均匀地包含 1200 个音分。因此，相邻两个音分之间的频率倍数应当为$2^{\frac{1}{1200}}$ 。
音程为 1 音分两个音符，人耳是很难分辨的。按照 [I. Peretz 和 K.L. Hyde的说法](https://web.archive.org/web/20100401073629/http://www.brams.umontreal.ca/plab/downloads/PeretzHyde03.pdf)，普通成人可以清晰分辨的最短音程差至少是 25 音分。而患有乐感丧失症的成人（一般意义上的五音不全者即属此类），不能分辨小于 100 音分的音程差。
## 损失函数结构
要明白这个损失函数，首先要明白音分是什么？音程是什么？音高是什么？
注意上面模型的输出是一个长度为360的一维数据，输出层中的每一个节点对应一个音分$C$，其中每个节点代表一个覆盖20音分的频率区间。也就是说，360个节点一共代表了7200音分的音程。  
同时需要注意的是每个音分都是和频率一一对应的，音分与频率关系的表达式为
$$C=1200log_2\frac{f}{f_{ref}},f_{ref}=10Hz$$
比如第1个节点，可以认为是音分为2000，对应频率约为31.748Hz。由于每个节点之间音程为20个音分，所以很容易知道，第201个节点对应的音分为6000，对应的频率约为320Hz  
由此360个节点代表的音分范围为2000到9180，正好频率可以覆盖C1到B7音高，意味着在32.70 Hz（C1）到1975.5 Hz（B7）的范围内，每20音分就有一个输出值。

在理想情况下，我们会想让360个节点的输出只有基频所在的维度置信度为1，其余均趋近于0，但是实际上这样并符合于实际的分布，因为频率是落在某一个区间内，不可能正好就是某个值，也不利于神经网络进行拟合，实际上我们是这么取目标值置信度的
$$
 y_i = \exp \left( -\frac{(c_i - c_{\text{true}})^2}{2 \cdot 25^2} \right) 
$$
也就是说我们让真实的置信度符合一个高斯分布，如上面的式子那样，  
这样设计是有意引导神经网络的输出符合高斯分布。按照文章中所说的，这是为了减轻对接近正确预测的惩罚，目标值在频率上进行了高斯模糊处理，使得围绕真实频率的能量以25音分的标准差衰减。

然后损失函数使用交叉熵损失函数
$$
\mathcal{L}(\mathbf{y}, \hat{\mathbf{y}}) = \sum_{i=1}^{360} \left( -y_i \log \hat{y}_i - (1 - y_i) \log (1 - \hat{y}_i) \right)
$$
注意预测值和目标值代表的都是置信度！！！范围为0到1，但是不能真的把这个东西看作是一个概率密度函数。

论文中使用了ADAM作为作为优化器，学习率为0.0002  
论文中训练了32个epoch，batch size为32，总共16000个样本。
# 数据集处理
论文中数据集第一个来源于RWC-synth，这是一个基于基频人工合成的数据集？  
但是论文中也说了这个数据集明显的缺点，那就是过于单一，而且高度同质化。这个数据集貌似就是多个正弦函数的叠加。  
第二个来源靠谱一点，来源于MedleyDB，但是也进行了部分的重新生成，使得基频更加准确？第二个数据集就丰富了很多，包含了25种乐器的声音。  

论文中提到使用5-fold交叉验证，并将数据集按6/2/2的比例划分为训练集、验证集和测试集。
## 人工生成数据集
首先就是要确定一个基频，然后添加三个共振峰，然后在添加上噪声，但是需要注意的是如果波形没有噪声的时候，使用傅里叶方法还能对频谱进行分析得到准确的基频，但是一旦噪声超过限制，基本上频谱就惨不忍睹了。  
虽然有噪声的话基本上别的方法也不行  

还有就是五个波形是按照不同权重进行相加的，一般来说基频权重最大，然后噪声权重最小（一般是小很多），要不然真没法预测。

事实证明，想要拟合这个合成的数据集简直就是太简单了，10个epoch就已经接近完美拟合了，我觉得需要更多更好的数据集。


## MIR-1K数据集
需要注意的是MIR-1K以20ms为一帧进行的标注。
音高以MIDI标准进行标注，从0到127分别表示不同的音高
昨天睡觉前发现了一个噩耗这个数据集是分声道的，两个声道分别是歌声和背景音乐，结果我没注意，给融合了，哎18号的晚上的工作算是白干了。。。。

事先说明，我没有进行音频数据清洗，就是单纯的把MIR-1K的音频数据进行了切片，去掉了基频变化过大的地方（往往都是有声音和无声的分界点）。正确分离了声道（分离了人声和配乐），提取了好的数据集以后，发现训练效果果然没问题了。  但是还是发现了一个问题。
但是我在使用MIR-1K以外的音频数据进行测试的时候发现，貌似MIR-1K的数据集过于干净了，尤其是在频谱上体现的更加明显，MIR-1K切片出来的数据普遍都是频谱非常干净的那种，但是MIR-1K以外的音频数据频谱就不太好，基频附近的频率分量非常多。我怀疑主要是因为MIR-1K的数据都是唱歌的过程中得来的？但是我用于测试的MIR-1K以外的音频数据是念的所以会出现问题？

经过我多个音频的测试，发现正常情况下的人声音频数据，模型表现都是正常的，如果在傅里叶频谱上看也是很正常的，是我用来测试的那个音频不太行，只是个别现象罢了，但是我还是发现了模型会出现性能不佳的几个情况：
1. 在频域上，基频分量过于微弱，跟噪声一个量级了
2. 有其他基频的杂音时

# 傅里叶分析
当采样率为16kHz时候，如果采用N点数据点，进行傅里叶分析，那么傅里叶两个分量波之间频率差距为1s除以样本实际时长，在相同采样率的情况下，也就说采样点越多傅里叶分析的频率精细度越高。
这也是为什么开窗傅里叶分析不能用于基频分析的直接原因，因为采样数越多，窗口时长越大，但是精度高，采样数越小，窗口时长越小，精度越低。

但是同时注意到因为生成16kHz的音频的时候，基频以及共振峰的频率精度为1，都是1的倍数，没有小数部分，所以其实，16000样本点的FFT就足以得到100%准确的基频以及共振峰数据了。

为什么不直接把傅里叶频谱当作特征数据进行训练呢？
我觉得这样还是无法克服傅里叶窗口法检测基频的缺点。因为对于傅里叶频谱虽然其能够完美的还原信号，但是需要注意的是无法尝试直接从一个频谱中获得一个准确的基频，可以尝试以下对一个仅仅由简单的几个正弦波叠加而成的频谱数据进行采样，无法保证基频一定落在频谱的频率分量上，导致会有很大的误差。**这是无法避免的误差**  
但是我觉得使用傅里叶频谱进行一定的滤波应该是没有问题。
# CREPE原版试用
这个需要使用tensorflow才行

试用失败，那个包下载不了，

同时在这里我不得不再次吐槽一波tensorflow，人家pytorch能做到pip安装无障碍开箱即用，tensorflow一安装好了，import就开始报错，真的